{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from optuna.trial import TrialState\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import  mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from optuna.trial import TrialState\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set this value to true if hyperparameter tuning is complete and the test set should be loaded and predicted on\n",
    "OUTPUT_TEST = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other parameters for NN training\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "LOSS_FN = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the training and validation datasets\n",
    "X_train = pd.read_csv(\"../data/cleaned/training.csv\")\n",
    "y_train = pd.read_csv(\"../data/cleaned/training_labels.csv\")\n",
    "X_val = pd.read_csv(\"../data/cleaned/validation.csv\")\n",
    "y_val = pd.read_csv(\"../data/cleaned/validation_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some columns headers contain '[' or ']' which are not compatable with sklearn. They are change to '(' and ')' respectively.\n",
    "columns = X_train.columns\n",
    "for col in columns:\n",
    "    if '[' in col or ']' in col:\n",
    "        old_name = col\n",
    "        col = col.replace('[', '(')\n",
    "        col = col.replace(']', ')')\n",
    "        \n",
    "        X_train = X_train.rename(columns={old_name:col})\n",
    "        X_val = X_val.rename(columns={old_name:col})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting of the training set into a vedrification and training set with a 90/10 split. This verification set is used for optuna hyperparameter tuning.\n",
    "X_train, X_verif, y_train, y_verif = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset the indicies after splitting the dataset\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "X_verif = X_verif.reset_index(drop=True)\n",
    "y_verif = y_verif.reset_index(drop=True)\n",
    "X_val = X_val.reset_index(drop=True)\n",
    "y_test = y_val.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_SUM = sum(y_train.to_numpy().squeeze())\n",
    "print(Y_SUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_loss(true, pred):\n",
    "    n = len(true)\n",
    "    weights = true / torch.sum(true)\n",
    "    squared = (true - pred)**2\n",
    "    #df_1 = true.detach().numpy().squeeze()\n",
    "    #df_2 = weights.detach().numpy().squeeze()\n",
    "    #df_3 = squared.detach().numpy().squeeze()\n",
    "    #weights_df = pd.DataFrame({'vals':df_1,'weight':df_2,'squared':df_3})\n",
    "    #print(weights_df.head())\n",
    "    weighted = squared * weights\n",
    "    loss = torch.sum(weighted) / n\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS_FN = weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create custom dataset function for perovsktie solar cell database\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features_dataframe, target_dataframe):\n",
    "        self.features = features_dataframe\n",
    "        self.target = target_dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract features and target for the given index\n",
    "        features = torch.tensor(self.features.iloc[idx].values, dtype=torch.float32)\n",
    "        target = torch.tensor(self.target.iloc[idx].values, dtype=torch.float32)\n",
    "        return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(features_dataframe=X_train, target_dataframe=y_train)\n",
    "verif_dataset = CustomDataset(features_dataframe=X_verif, target_dataframe=y_verif)\n",
    "val_dataset = CustomDataset(features_dataframe=X_val, target_dataframe=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "verif_dataloader = DataLoader(verif_dataset, batch_size=BATCH_SIZE)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose training hardware based on what is locally available\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(trial):\n",
    "    # We optimize the number of layers, hidden units and dropout ratio in each layer.\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 4,6)\n",
    "    layers = []\n",
    "\n",
    "    input = len(X_train.columns)\n",
    "    in_features = input\n",
    "    for i in range(n_layers):\n",
    "        out_features = trial.suggest_int(\"n_units_l{}\".format(i), 10, input//2)\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        p = trial.suggest_float(\"dropout_l{}\".format(i), 0.2, 0.5)\n",
    "        layers.append(nn.Dropout(p))\n",
    "\n",
    "        in_features = out_features\n",
    "    layers.append(nn.Linear(in_features, 1))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Generate the model.\n",
    "    model = define_model(trial).to(device)\n",
    "    print(model)\n",
    "    \n",
    "    # Generate the optimizers.\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "    # Training of the model.\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "        model.train()\n",
    "        for batch_idx, (X, y) in enumerate(train_dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "            loss = LOSS_FN(pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            #nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validation of the model.\n",
    "        model.eval()\n",
    "        test_loss, avg_error = 0, 0\n",
    "        verif_error = 0\n",
    "        verif_loss = 0\n",
    "        num_batches = len(verif_dataloader)\n",
    "        verif_size = len(verif_dataset)\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (X, y) in enumerate(verif_dataloader):\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                pred = model(X)\n",
    "                test_loss += LOSS_FN(pred, y).item()\n",
    "                try:\n",
    "                    current_error = mean_squared_error(pred, y, squared=False)\n",
    "                    avg_error += current_error\n",
    "                    verif_squared_error = (pred - y)**2\n",
    "                    verif_summed_squared_error = torch.sum(verif_squared_error)\n",
    "                    verif_error += verif_summed_squared_error\n",
    "                    verif_loss += mean_absolute_percentage_error(y, pred)\n",
    "                except: #Some runs weights are believed to overflow causing predictions to all be NaN, a gradient clip layer is used to combat this but in some situations it is not enough and the trial must be pruned\n",
    "                    print(\"WARNING: Unstable MSE\")\n",
    "                    # Check for NaN values\n",
    "                    nan_mask = torch.isnan(pred)\n",
    "                    num_nan_entries = torch.sum(nan_mask).item()\n",
    "                    print(\"Prediction contains {num} NaN entries\".format(num=num_nan_entries))\n",
    "                    print(\"Pruning Trial\")\n",
    "                    raise optuna.exceptions.TrialPruned()\n",
    "                \n",
    "        test_loss /= num_batches #Output metric to gauge how model is doing as training happens\n",
    "        avg_error /= num_batches #Output metric to gauge how model is doing as training happens\n",
    "\n",
    "        verif_loss /= num_batches\n",
    "        verif_loss *= 100\n",
    "        verif_rmse = np.sqrt(verif_error / verif_size)\n",
    "\n",
    "        # Evaluate predictions\n",
    "        accuracy = verif_loss + verif_rmse\n",
    "        \n",
    "        print(f\"Test Error: \\nAvg RMSE: {avg_error}, Avg loss: {test_loss:>8f}\")\n",
    "        trial.report(accuracy, epoch)\n",
    "        print(f\"Optuna accuracy: {accuracy}\\n\")\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(pruner=optuna.pruners.SuccessiveHalvingPruner())\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        n_layers = params['n_layers']\n",
    "        layer_units = [params[f'n_units_l{i}'] for i in range(n_layers)]\n",
    "        layer_dropouts = [params[f'dropout_l{i}'] for i in range(n_layers)]\n",
    "\n",
    "        # Define layers based on the provided parameters\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(n_layers):\n",
    "            in_features = layer_units[i - 1] if i > 0 else len(X_train.columns)\n",
    "            out_features = layer_units[i]\n",
    "\n",
    "            self.layers.append(nn.Linear(in_features=in_features, out_features=out_features))\n",
    "            self.layers.append(nn.Dropout(layer_dropouts[i]))\n",
    "            \n",
    "            \n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(in_features=layer_units[-1], out_features=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            x = torch.relu(x)  # You can use other activation functions based on your task\n",
    "\n",
    "        # Output layer\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = trial.params\n",
    "params = {'n_layers': 1, 'n_units_l0': 4147, 'dropout_l0': 0.36, 'optimizer': 'RMSprop', 'lr': 0.0002615}\n",
    "model = NeuralNetwork(params=params)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create optimizer based on optuna results\n",
    "op_name = params['optimizer']\n",
    "if op_name == 'Adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "elif op_name == 'RMSprop':\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=params['lr'])\n",
    "elif op_name == 'SGD':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=params['lr'])\n",
    "else:\n",
    "    raise ValueError(\"Optimizer name not found. Ensure it is added to the list above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=params['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        #print(loss.detach().numpy())\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        #nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 9 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    preds = []\n",
    "    true = []\n",
    "    model.eval()\n",
    "    test_loss, error = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            preds.append(list(pred.numpy()))\n",
    "            true.append(list(y.numpy()))\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            error += mean_squared_error(pred, y, squared=False)\n",
    "    test_loss /= num_batches\n",
    "    error /= num_batches\n",
    "    print(f\"Test Error: \\nAvg RMSE: {error}, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return preds, true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(EPOCHS):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, LOSS_FN, optimizer)\n",
    "    test(val_dataloader, model, LOSS_FN)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds, train_true = test(train_dataloader, model, LOSS_FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = np.concatenate(train_preds).ravel()\n",
    "train_true = np.concatenate(train_true).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds, val_true = test(val_dataloader, model, LOSS_FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = np.concatenate(val_preds).ravel()\n",
    "val_true = np.concatenate(val_true).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_rmse = mean_squared_error(val_true,val_preds,squared=False)\n",
    "val_r = r2_score(val_true,val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_rmse)\n",
    "print(val_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not OUTPUT_TEST:\n",
    "    raise ValueError(\"OUTPUT_TEST set to False. If you would like to output final test values set to True and continue running from here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(\"../data/cleaned/test.csv\")\n",
    "y_test = pd.read_csv(\"../data/cleaned/test_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = X_test.columns\n",
    "for col in columns:\n",
    "    if '[' in col or ']' in col:\n",
    "        old_name = col\n",
    "        col = col.replace('[', '(')\n",
    "        col = col.replace(']', ')')\n",
    "        \n",
    "        X_test = X_test.rename(columns={old_name:col})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(features_dataframe=X_test, target_dataframe=y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred, test_true = test(test_dataloader, model, LOSS_FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = np.concatenate(test_pred).ravel()\n",
    "test_true = np.concatenate(test_true).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = mean_squared_error(test_true,test_pred,squared=False)\n",
    "print(\"RMSE:\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save test true vals and predictions to csv\n",
    "\n",
    "pred_data = pd.DataFrame(test_pred)\n",
    "pred_filepath = '../data/predictions/NN/test_pred_nn.csv'\n",
    "pred_data.to_csv(pred_filepath, index=False, header=False)\n",
    "pred_data = pd.DataFrame(test_true)\n",
    "pred_filepath = '../data/predictions/NN/test_true_nn.csv'\n",
    "pred_data.to_csv(pred_filepath, index=False, header=False)\n",
    "\n",
    "#Save train true vals and predictions to csv\n",
    "\n",
    "pred_data = pd.DataFrame(train_preds)\n",
    "pred_filepath = '../Data/Predictions/NN/train_pred_nn.csv'\n",
    "pred_data.to_csv(pred_filepath, index=False, header=False)\n",
    "pred_data = pd.DataFrame(train_true)\n",
    "pred_filepath = '../data/predictions/NN/train_true_nn.csv'\n",
    "pred_data.to_csv(pred_filepath, index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save inputs to csv\n",
    "\n",
    "pred_data = pd.DataFrame(X_train)\n",
    "pred_filepath = '../data/predictions/NN/train_input_nn.csv'\n",
    "pred_data.to_csv(pred_filepath, index=False, header=False)\n",
    "true_data = pd.DataFrame(X_test)\n",
    "true_filepath = '../data/predictions/NN/test_input_nn.csv'\n",
    "true_data.to_csv(true_filepath, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in values from csv and calculate RMSE and r values\n",
    "\n",
    "test_pred_data = np.genfromtxt('../data/predictions/NN/test_pred_nn.csv', delimiter=',', filling_values=np.nan)\n",
    "test_true_data = np.genfromtxt('../data/predictions/NN/test_true_nn.csv', delimiter=',', filling_values=np.nan)\n",
    "train_pred_data = np.genfromtxt('../data/predictions/NN/train_pred_nn.csv', delimiter=',', filling_values=np.nan)\n",
    "train_true_data = np.genfromtxt('../data/predictions/NN/train_true_nn.csv', delimiter=',', filling_values=np.nan)\n",
    "\n",
    "test_rmse = mean_squared_error(test_true_data,test_pred_data,squared=False)\n",
    "test_r = r2_score(test_true_data,test_pred_data)\n",
    "\n",
    "train_rmse = mean_squared_error(train_true_data,train_pred_data,squared=False)\n",
    "train_r = r2_score(train_true_data,train_pred_data)\n",
    "\n",
    "print(\"Train:\")\n",
    "print(train_rmse)\n",
    "print('Test:')\n",
    "print(test_rmse)\n",
    "print(test_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"percent Error:\", mean_absolute_percentage_error(test_true_data, test_pred_data)*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
