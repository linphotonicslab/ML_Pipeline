{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from optuna.trial import TrialState\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import  mean_squared_error\n",
    "from optuna.trial import TrialState\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_TEST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_TRUE_TEST = False\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "LOSS_FN = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"../data/cleaned/training.csv\")\n",
    "y_train = pd.read_csv(\"../data/cleaned/training_labels.csv\")\n",
    "X_val = pd.read_csv(\"../data/cleaned/validation.csv\")\n",
    "y_val = pd.read_csv(\"../data/cleaned/validation_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = X_train.columns\n",
    "for col in columns:\n",
    "    if '[' in col or ']' in col:\n",
    "        old_name = col\n",
    "        col = col.replace('[', '(')\n",
    "        col = col.replace(']', ')')\n",
    "        \n",
    "        X_train = X_train.rename(columns={old_name:col})\n",
    "        X_val = X_val.rename(columns={old_name:col})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_verif, y_train, y_verif = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "X_verif = X_verif.reset_index(drop=True)\n",
    "y_verif = y_verif.reset_index(drop=True)\n",
    "X_val = X_val.reset_index(drop=True)\n",
    "y_test = y_val.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Add sanity check for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features_dataframe, target_dataframe):\n",
    "        self.features = features_dataframe\n",
    "        self.target = target_dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract features and target for the given index\n",
    "        features = torch.tensor(self.features.iloc[idx].values, dtype=torch.float32)\n",
    "        target = torch.tensor(self.target.iloc[idx].values, dtype=torch.float32)\n",
    "        return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(features_dataframe=X_train, target_dataframe=y_train)\n",
    "verif_dataset = CustomDataset(features_dataframe=X_verif, target_dataframe=y_verif)\n",
    "val_dataset = CustomDataset(features_dataframe=X_val, target_dataframe=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "verif_dataloader = DataLoader(verif_dataset, batch_size=BATCH_SIZE)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(trial):\n",
    "    # We optimize the number of layers, hidden units and dropout ratio in each layer.\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    layers = []\n",
    "\n",
    "    input = 2808\n",
    "    in_features = input\n",
    "    for i in range(n_layers):\n",
    "        out_features = trial.suggest_int(\"n_units_l{}\".format(i), input, 2 * input)\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        p = trial.suggest_float(\"dropout_l{}\".format(i), 0.2, 0.5)\n",
    "        layers.append(nn.Dropout(p))\n",
    "\n",
    "        in_features = out_features\n",
    "    layers.append(nn.Linear(in_features, 1))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Generate the model.\n",
    "    model = define_model(trial).to(device)\n",
    "    print(model)\n",
    "    \n",
    "    # Generate the optimizers.\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "    # Training of the model.\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "        model.train()\n",
    "        train_error = 0\n",
    "        train_size = len(train_dataset)\n",
    "        for batch_idx, (X, y) in enumerate(train_dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            train_squared_error = (pred - y)**2\n",
    "            train_summed_squared_error = torch.sum(train_squared_error)\n",
    "            train_error += train_summed_squared_error\n",
    "\n",
    "        train_rmse = np.sqrt(train_error.detach().numpy() / train_size)\n",
    "        \n",
    "        # Validation of the model.\n",
    "        model.eval()\n",
    "        test_loss, avg_error = 0, 0\n",
    "        verif_error = 0\n",
    "        num_batches = len(verif_dataloader)\n",
    "        verif_size = len(verif_dataset)\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (X, y) in enumerate(verif_dataloader):\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                pred = model(X)\n",
    "                test_loss += loss_fn(pred, y).item()\n",
    "                try:\n",
    "                    current_error = mean_squared_error(pred, y, squared=False)\n",
    "                    avg_error += current_error\n",
    "                    verif_squared_error = (pred - y)**2\n",
    "                    verif_summed_squared_error = torch.sum(verif_squared_error)\n",
    "                    verif_error += verif_summed_squared_error\n",
    "                except:\n",
    "                    print(\"WARNING: Unstable MSE\")\n",
    "                    # Check for NaN values\n",
    "                    nan_mask = torch.isnan(pred)\n",
    "                    num_nan_entries = torch.sum(nan_mask).item()\n",
    "                    print(\"Prediction contains {num} NaN entries\".format(num=num_nan_entries))\n",
    "                    print(\"Pruning Trial\")\n",
    "                    raise optuna.exceptions.TrialPruned()\n",
    "                \n",
    "        test_loss /= num_batches #Output metric to gauge how model is doing as training happens\n",
    "        avg_error /= num_batches #Output metric to gauge how model is doing as training happens\n",
    "\n",
    "        verif_rmse = np.sqrt(verif_error / verif_size)\n",
    "\n",
    "        accuracy = abs(verif_rmse - train_rmse) + 2 * train_rmse #Metric for optuna to determine pruning and optimal hyperparameters\n",
    "        \n",
    "        print(f\"Test Error: \\nAvg RMSE: {avg_error}, Avg loss: {test_loss:>8f}\")\n",
    "        trial.report(accuracy, epoch)\n",
    "        print(f\"Optuna accuracy: {accuracy}\\n\")\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-11-30 18:23:56,053]\u001b[0m A new study created in memory with name: no-name-5e03ecf9-e0ce-4446-b079-f979e9c5cc03\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=2808, out_features=8, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.49234418479137676, inplace=False)\n",
      "  (3): Linear(in_features=8, out_features=7, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.381199014716318, inplace=False)\n",
      "  (6): Linear(in_features=7, out_features=1, bias=True)\n",
      ")\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Test Error: \n",
      "Avg RMSE: 13.106254471672905, Avg loss: 171.919505\n",
      "Optuna accuracy: 139.4290771484375\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Test Error: \n",
      "Avg RMSE: 10.330003632439507, Avg loss: 106.827569\n",
      "Optuna accuracy: 26.2977237701416\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Test Error: \n",
      "Avg RMSE: 8.363110542297363, Avg loss: 70.031101\n",
      "Optuna accuracy: 21.396831512451172\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Test Error: \n",
      "Avg RMSE: 6.186588128407796, Avg loss: 38.334899\n",
      "Optuna accuracy: 16.691936492919922\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Test Error: \n",
      "Avg RMSE: 5.362303680843777, Avg loss: 28.824481\n",
      "Optuna accuracy: 12.798941612243652\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Test Error: \n",
      "Avg RMSE: 4.637167771657308, Avg loss: 21.668534\n",
      "Optuna accuracy: 10.703819274902344\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Test Error: \n",
      "Avg RMSE: 4.57674437099033, Avg loss: 21.149633\n",
      "Optuna accuracy: 9.621435165405273\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Test Error: \n",
      "Avg RMSE: 4.39393417040507, Avg loss: 19.580829\n",
      "Optuna accuracy: 9.38969898223877\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Test Error: \n",
      "Avg RMSE: 4.372218582365248, Avg loss: 19.410755\n",
      "Optuna accuracy: 9.074986457824707\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-11-30 18:24:03,162]\u001b[0m Trial 0 finished with value: 9.033117294311523 and parameters: {'n_layers': 2, 'n_units_l0': 8, 'dropout_l0': 0.49234418479137676, 'n_units_l1': 7, 'dropout_l1': 0.381199014716318, 'optimizer': 'RMSprop', 'lr': 0.07657462938391439}. Best is trial 0 with value: 9.033117294311523.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      "Avg RMSE: 4.3789621988932295, Avg loss: 19.454231\n",
      "Optuna accuracy: 9.033117294311523\n",
      "\n",
      "Sequential(\n",
      "  (0): Linear(in_features=2808, out_features=10, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.4591783975221717, inplace=False)\n",
      "  (3): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.32622011638240467, inplace=False)\n",
      "  (6): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Test Error: \n",
      "Avg RMSE: 11.75962405734592, Avg loss: 139.246997\n",
      "Optuna accuracy: 36.007286071777344\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Test Error: \n",
      "Avg RMSE: 10.587979740566677, Avg loss: 112.911240\n",
      "Optuna accuracy: 22.93268585205078\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Test Error: \n",
      "Avg RMSE: 10.244612375895182, Avg loss: 105.721160\n",
      "Optuna accuracy: 20.358694076538086\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Test Error: \n",
      "Avg RMSE: 9.746447245279947, Avg loss: 95.665655\n",
      "Optuna accuracy: 19.28675651550293\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Test Error: \n",
      "Avg RMSE: 9.468975490993923, Avg loss: 90.142635\n",
      "Optuna accuracy: 18.583267211914062\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Test Error: \n",
      "Avg RMSE: 8.967477586534288, Avg loss: 80.812492\n",
      "Optuna accuracy: 17.55892562866211\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Test Error: \n",
      "Avg RMSE: 8.127452161577013, Avg loss: 66.391275\n",
      "Optuna accuracy: 16.22513198852539\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Test Error: \n",
      "Avg RMSE: 7.672386699252659, Avg loss: 59.061992\n",
      "Optuna accuracy: 15.596036911010742\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Test Error: \n",
      "Avg RMSE: 6.840533256530762, Avg loss: 46.940860\n",
      "Optuna accuracy: 14.684005737304688\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-11-30 18:24:10,533]\u001b[0m Trial 1 finished with value: 14.312532424926758 and parameters: {'n_layers': 2, 'n_units_l0': 10, 'dropout_l0': 0.4591783975221717, 'n_units_l1': 10, 'dropout_l1': 0.32622011638240467, 'optimizer': 'Adam', 'lr': 0.0024146102516713337}. Best is trial 0 with value: 9.033117294311523.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      "Avg RMSE: 5.980509069230822, Avg loss: 35.895510\n",
      "Optuna accuracy: 14.312532424926758\n",
      "\n",
      "Study statistics: \n",
      "  Number of finished trials:  2\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  2\n",
      "Best trial:\n",
      "  Value:  9.033117294311523\n",
      "  Params: \n",
      "    n_layers: 2\n",
      "    n_units_l0: 8\n",
      "    dropout_l0: 0.49234418479137676\n",
      "    n_units_l1: 7\n",
      "    dropout_l1: 0.381199014716318\n",
      "    optimizer: RMSprop\n",
      "    lr: 0.07657462938391439\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(pruner=optuna.pruners.SuccessiveHalvingPruner())\n",
    "study.optimize(objective, n_trials=2)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        n_layers = params['n_layers']\n",
    "        layer_units = [params[f'n_units_l{i}'] for i in range(n_layers)]\n",
    "        layer_dropouts = [params[f'dropout_l{i}'] for i in range(n_layers)]\n",
    "\n",
    "        # Define layers based on the provided parameters\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(n_layers):\n",
    "            in_features = layer_units[i - 1] if i > 0 else 2808\n",
    "            out_features = layer_units[i]\n",
    "\n",
    "            self.layers.append(nn.Linear(in_features=in_features, out_features=out_features))\n",
    "            self.layers.append(nn.Dropout(layer_dropouts[i]))\n",
    "            self.layers.append(nn.BatchNorm1d(out_features))\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(in_features=layer_units[-1], out_features=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            x = torch.relu(x)  # You can use other activation functions based on your task\n",
    "\n",
    "        # Output layer\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=2808, out_features=8, bias=True)\n",
      "    (1): Dropout(p=0.49234418479137676, inplace=False)\n",
      "    (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Linear(in_features=8, out_features=7, bias=True)\n",
      "    (4): Dropout(p=0.381199014716318, inplace=False)\n",
      "    (5): BatchNorm1d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=7, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "params = trial.params\n",
    "model = NeuralNetwork(params=params)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_name = params['optimizer']\n",
    "if op_name == 'Adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "elif op_name == 'RMSprop':\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=params['lr'])\n",
    "elif op_name == 'SGD':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=params['lr'])\n",
    "else:\n",
    "    raise ValueError(\"Optimizer name not found. Ensure it is added to the list above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 9 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    preds = []\n",
    "    true = []\n",
    "    model.eval()\n",
    "    test_loss, error = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            preds.append(list(pred.numpy()))\n",
    "            true.append(list(y.numpy()))\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            error += mean_squared_error(pred, y, squared=False)\n",
    "    test_loss /= num_batches\n",
    "    error /= num_batches\n",
    "    print(f\"Test Error: \\nAvg RMSE: {error}, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return preds, true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 345.594543  [   64/ 5150]\n",
      "loss: 32.515858  [  640/ 5150]\n",
      "loss: 23.879959  [ 1216/ 5150]\n",
      "loss: 14.749730  [ 1792/ 5150]\n",
      "loss: 19.936762  [ 2368/ 5150]\n",
      "loss: 11.156807  [ 2944/ 5150]\n",
      "loss: 23.845108  [ 3520/ 5150]\n",
      "loss: 22.476679  [ 4096/ 5150]\n",
      "loss: 27.426199  [ 4672/ 5150]\n",
      "Test Error: \n",
      "Avg RMSE: 4.2601378957430525, Avg loss: 18.829528 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 24.125467  [   64/ 5150]\n",
      "loss: 15.041159  [  640/ 5150]\n",
      "loss: 19.356709  [ 1216/ 5150]\n",
      "loss: 13.431541  [ 1792/ 5150]\n",
      "loss: 19.943216  [ 2368/ 5150]\n",
      "loss: 10.170897  [ 2944/ 5150]\n",
      "loss: 22.797131  [ 3520/ 5150]\n",
      "loss: 21.633530  [ 4096/ 5150]\n",
      "loss: 27.035475  [ 4672/ 5150]\n",
      "Test Error: \n",
      "Avg RMSE: 4.225183506806691, Avg loss: 18.482717 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 24.526281  [   64/ 5150]\n",
      "loss: 15.641088  [  640/ 5150]\n",
      "loss: 18.050133  [ 1216/ 5150]\n",
      "loss: 12.800880  [ 1792/ 5150]\n",
      "loss: 19.714518  [ 2368/ 5150]\n",
      "loss: 11.350188  [ 2944/ 5150]\n",
      "loss: 21.793854  [ 3520/ 5150]\n",
      "loss: 20.362823  [ 4096/ 5150]\n",
      "loss: 26.778469  [ 4672/ 5150]\n",
      "Test Error: \n",
      "Avg RMSE: 4.21296348174413, Avg loss: 18.334125 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 23.545906  [   64/ 5150]\n",
      "loss: 15.067343  [  640/ 5150]\n",
      "loss: 17.923908  [ 1216/ 5150]\n",
      "loss: 14.835665  [ 1792/ 5150]\n",
      "loss: 19.891119  [ 2368/ 5150]\n",
      "loss: 10.238291  [ 2944/ 5150]\n",
      "loss: 20.716780  [ 3520/ 5150]\n",
      "loss: 19.829515  [ 4096/ 5150]\n",
      "loss: 27.382294  [ 4672/ 5150]\n",
      "Test Error: \n",
      "Avg RMSE: 4.170522669951121, Avg loss: 17.959408 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 22.948502  [   64/ 5150]\n",
      "loss: 14.620034  [  640/ 5150]\n",
      "loss: 18.962620  [ 1216/ 5150]\n",
      "loss: 12.750851  [ 1792/ 5150]\n",
      "loss: 18.863461  [ 2368/ 5150]\n",
      "loss: 9.774944  [ 2944/ 5150]\n",
      "loss: 20.523924  [ 3520/ 5150]\n",
      "loss: 19.596937  [ 4096/ 5150]\n",
      "loss: 26.858414  [ 4672/ 5150]\n",
      "Test Error: \n",
      "Avg RMSE: 4.160757124423981, Avg loss: 17.849636 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 22.413612  [   64/ 5150]\n",
      "loss: 14.497065  [  640/ 5150]\n",
      "loss: 17.093124  [ 1216/ 5150]\n",
      "loss: 11.062158  [ 1792/ 5150]\n",
      "loss: 19.136127  [ 2368/ 5150]\n",
      "loss: 8.903694  [ 2944/ 5150]\n",
      "loss: 20.342302  [ 3520/ 5150]\n",
      "loss: 19.564793  [ 4096/ 5150]\n",
      "loss: 25.809484  [ 4672/ 5150]\n",
      "Test Error: \n",
      "Avg RMSE: 4.135058144728343, Avg loss: 17.643414 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 21.452343  [   64/ 5150]\n",
      "loss: 13.772321  [  640/ 5150]\n",
      "loss: 16.491816  [ 1216/ 5150]\n",
      "loss: 10.664833  [ 1792/ 5150]\n",
      "loss: 19.105474  [ 2368/ 5150]\n",
      "loss: 10.095732  [ 2944/ 5150]\n",
      "loss: 20.153744  [ 3520/ 5150]\n",
      "loss: 19.293392  [ 4096/ 5150]\n",
      "loss: 25.566912  [ 4672/ 5150]\n",
      "Test Error: \n",
      "Avg RMSE: 4.132879575093587, Avg loss: 17.658025 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 21.831007  [   64/ 5150]\n",
      "loss: 14.687248  [  640/ 5150]\n",
      "loss: 17.537222  [ 1216/ 5150]\n",
      "loss: 11.630647  [ 1792/ 5150]\n",
      "loss: 19.193567  [ 2368/ 5150]\n",
      "loss: 10.644114  [ 2944/ 5150]\n",
      "loss: 19.898537  [ 3520/ 5150]\n",
      "loss: 19.329325  [ 4096/ 5150]\n",
      "loss: 26.012714  [ 4672/ 5150]\n",
      "Test Error: \n",
      "Avg RMSE: 4.095104277133942, Avg loss: 17.297262 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 23.579641  [   64/ 5150]\n",
      "loss: 15.230037  [  640/ 5150]\n",
      "loss: 15.914334  [ 1216/ 5150]\n",
      "loss: 10.759924  [ 1792/ 5150]\n",
      "loss: 18.570166  [ 2368/ 5150]\n",
      "loss: 8.790736  [ 2944/ 5150]\n",
      "loss: 20.452570  [ 3520/ 5150]\n",
      "loss: 19.821173  [ 4096/ 5150]\n",
      "loss: 25.740200  [ 4672/ 5150]\n",
      "Test Error: \n",
      "Avg RMSE: 4.141437927881877, Avg loss: 17.688242 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 21.611235  [   64/ 5150]\n",
      "loss: 14.158173  [  640/ 5150]\n",
      "loss: 16.611202  [ 1216/ 5150]\n",
      "loss: 10.694131  [ 1792/ 5150]\n",
      "loss: 19.025896  [ 2368/ 5150]\n",
      "loss: 8.622880  [ 2944/ 5150]\n",
      "loss: 20.749683  [ 3520/ 5150]\n",
      "loss: 19.129490  [ 4096/ 5150]\n",
      "loss: 25.621338  [ 4672/ 5150]\n",
      "Test Error: \n",
      "Avg RMSE: 4.150431513786316, Avg loss: 17.758946 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for t in range(EPOCHS):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(val_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      "Avg RMSE: 4.198883683593185, Avg loss: 18.096174 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_preds, train_true = test(train_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = np.concatenate(train_preds).ravel()\n",
    "train_true = np.concatenate(train_true).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not OUTPUT_TEST:\n",
    "    raise ValueError(\"OUTPUT_TEST set to False. If you would like to output final test values set to True and continue running from here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(\"../data/cleaned/test.csv\")\n",
    "y_test = pd.read_csv(\"../data/cleaned/test_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = X_test.columns\n",
    "for col in columns:\n",
    "    if '[' in col or ']' in col:\n",
    "        old_name = col\n",
    "        col = col.replace('[', '(')\n",
    "        col = col.replace(']', ')')\n",
    "        \n",
    "        X_test = X_test.rename(columns={old_name:col})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(features_dataframe=X_test, target_dataframe=y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      "Avg RMSE: 4.530055443445842, Avg loss: 20.935884 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pred, test_true = test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = np.concatenate(test_pred).ravel()\n",
    "test_true = np.concatenate(test_true).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 4.6317825\n"
     ]
    }
   ],
   "source": [
    "error = mean_squared_error(test_true,test_pred,squared=False)\n",
    "print(\"RMSE:\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save test true vals and predictions to csv\n",
    "\n",
    "pred_data = pd.DataFrame(test_pred)\n",
    "pred_filepath = '../data/predictions/NN/test_pred_nn.csv'\n",
    "pred_data.to_csv(pred_filepath, index=False, header=False)\n",
    "pred_data = pd.DataFrame(test_true)\n",
    "pred_filepath = '../data/predictions/NN/test_true_nn.csv'\n",
    "pred_data.to_csv(pred_filepath, index=False, header=False)\n",
    "\n",
    "#Save train true vals and predictions to csv\n",
    "\n",
    "pred_data = pd.DataFrame(train_preds)\n",
    "pred_filepath = '../Data/Predictions/NN/train_pred_nn.csv'\n",
    "pred_data.to_csv(pred_filepath, index=False, header=False)\n",
    "pred_data = pd.DataFrame(train_true)\n",
    "pred_filepath = '../data/predictions/NN/train_true_nn.csv'\n",
    "pred_data.to_csv(pred_filepath, index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save inputs to csv\n",
    "\n",
    "pred_data = pd.DataFrame(X_train)\n",
    "pred_filepath = '../data/predictions/NN/train_input_nn.csv'\n",
    "pred_data.to_csv(pred_filepath, index=False, header=False)\n",
    "true_data = pd.DataFrame(X_test)\n",
    "true_filepath = '../data/predictions/NN/test_input_nn.csv'\n",
    "true_data.to_csv(true_filepath, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "4.252383160967171\n",
      "Test:\n",
      "4.631782652544545\n",
      "PearsonRResult(statistic=0.3516642696602043, pvalue=2.8666309231772304e-22)\n"
     ]
    }
   ],
   "source": [
    "#Read in values from csv and calculate RMSE and r values\n",
    "\n",
    "test_pred_data = np.genfromtxt('../data/predictions/NN/test_pred_nn.csv', delimiter=',', filling_values=np.nan)\n",
    "test_true_data = np.genfromtxt('../data/predictions/NN/test_true_nn.csv', delimiter=',', filling_values=np.nan)\n",
    "train_pred_data = np.genfromtxt('../data/predictions/NN/train_pred_nn.csv', delimiter=',', filling_values=np.nan)\n",
    "train_true_data = np.genfromtxt('../data/predictions/NN/train_true_nn.csv', delimiter=',', filling_values=np.nan)\n",
    "\n",
    "test_rmse = mean_squared_error(test_true_data,test_pred_data,squared=False)\n",
    "test_r = stats.pearsonr(test_true_data,test_pred_data)\n",
    "\n",
    "train_rmse = mean_squared_error(train_true_data,train_pred_data,squared=False)\n",
    "train_r = stats.pearsonr(train_true_data,train_pred_data)\n",
    "\n",
    "print(\"Train:\")\n",
    "print(train_rmse)\n",
    "print('Test:')\n",
    "print(test_rmse)\n",
    "print(test_r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
