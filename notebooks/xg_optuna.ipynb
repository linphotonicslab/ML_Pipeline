{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import  mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from scipy import stats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set this value to true if hyperparameter tuning is complete and the test set should be loaded and predicted on\n",
    "OUTPUT_TEST = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the training and validation datasets\n",
    "X_train = pd.read_csv(\"../data/cleaned/training.csv\")\n",
    "y_train = pd.read_csv(\"../data/cleaned/training_labels.csv\")\n",
    "X_val = pd.read_csv(\"../data/cleaned/validation.csv\")\n",
    "y_val = pd.read_csv(\"../data/cleaned/validation_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some columns headers contain '[' or ']' which are not compatable with sklearn. They are change to '(' and ')' respectively.\n",
    "columns = X_train.columns\n",
    "for col in columns:\n",
    "    if '[' in col or ']' in col:\n",
    "        old_name = col\n",
    "        col = col.replace('[', '(')\n",
    "        col = col.replace(']', ')')\n",
    "        \n",
    "        X_train = X_train.rename(columns={old_name:col})\n",
    "        X_val = X_val.rename(columns={old_name:col})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting of the training set into a vedrification and training set with a 90/10 split. This verification set is used for optuna hyperparameter tuning.\n",
    "X_train, X_verif, y_train, y_verif = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset the indicies after splitting the dataset\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "X_verif = X_verif.reset_index(drop=True)\n",
    "y_verif = y_verif.reset_index(drop=True)\n",
    "X_val = X_val.reset_index(drop=True)\n",
    "y_val = y_val.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check performance with no tuning to ensure performance is improving\n",
    "sanity_check = XGBRegressor()\n",
    "sanity_check.fit(X_train, y_train)\n",
    "val_preds = sanity_check.predict(X_val)\n",
    "sanity_val_error = mean_squared_error(y_val,val_preds,squared=False)\n",
    "val_true = y_val.to_numpy().squeeze()\n",
    "sanity_val_r = r2_score(val_true,val_preds)[0]\n",
    "print(\"SANITY CHECK VALUES:\")\n",
    "print(\"Validation RMSE:\", sanity_val_error)\n",
    "print(\"Validation R:\", sanity_val_r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial):\n",
    "    #Define the objective function\n",
    "\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 15),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 1.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0),\n",
    "        'subsample': trial.suggest_loguniform('subsample', 0.5, 0.9),\n",
    "        'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.5, 0.9),\n",
    "    }\n",
    "\n",
    "    params[\"tree_method\"] = \"hist\"\n",
    "\n",
    "    # Fit the model\n",
    "    optuna_model = XGBRegressor(**params)\n",
    "    optuna_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    verif_pred = optuna_model.predict(X_verif)\n",
    "    verif_loss = mean_absolute_percentage_error(y_verif,verif_pred)*100\n",
    "    verif_error = mean_squared_error(y_verif,verif_pred,squared=False)\n",
    "\n",
    "    # Evaluate predictions\n",
    "    error = verif_loss + verif_error\n",
    "    \n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = optuna.samplers.CmaEsSampler()\n",
    "study = optuna.create_study(sampler=sampler)\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = trial.params\n",
    "#params = {'max_depth': 8, 'learning_rate': 0.32451199000475434, 'n_estimators': 525, 'min_child_weight': 6, 'gamma': 6.149852814458083e-05, 'subsample': 0.8924095316799702, 'colsample_bytree': 0.7277371421020629}\n",
    "model = XGBRegressor(**params)\n",
    "print(params)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "\n",
    "val_preds = model.predict(X_val)\n",
    "train_preds = model.predict(X_train)\n",
    "error = mean_squared_error(y_val,val_preds,squared=False)\n",
    "r_error = r2_score(val_true,val_preds)[0]\n",
    "train_error = mean_squared_error(y_train, train_preds)\n",
    "train_true = y_train.to_numpy().squeeze()\n",
    "train_r_error = r2_score(train_true, train_preds)[0]\n",
    "print(\"Validation RMSE:\", error)\n",
    "print(\"Difference from sanity check:\", error - sanity_val_error)\n",
    "print(\"Validation R:\", r_error)\n",
    "print(\"Difference from sanity check:\", r_error - sanity_val_r)\n",
    "print(\"Validation PE\", mean_absolute_percentage_error(val_true, val_preds))\n",
    "\n",
    "print(\"Training RMSE:\", train_error)\n",
    "print(\"Training R:\", train_r_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not OUTPUT_TEST:\n",
    "    raise ValueError(\"OUTPUT_TEST set to False. If you would like to output final test values set to True and continue running from here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(\"../data/cleaned/test.csv\")\n",
    "y_test = pd.read_csv(\"../data/cleaned/test_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = X_test.columns\n",
    "for col in columns:\n",
    "    if '[' in col or ']' in col:\n",
    "        old_name = col\n",
    "        col = col.replace('[', '(')\n",
    "        col = col.replace(']', ')')\n",
    "        \n",
    "        X_test = X_test.rename(columns={old_name:col})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.predict(X_test)\n",
    "train_preds = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save test true vals and predictions to csv\n",
    "\n",
    "pred_data = pd.DataFrame(test_preds)\n",
    "pred_filepath = '../data/predictions/XG/test_pred_xg.csv'\n",
    "pred_data.to_csv(pred_filepath, index=False, header=False)\n",
    "pred_data = pd.DataFrame(y_test)\n",
    "pred_filepath = '../data/predictions/XG/test_true_xg.csv'\n",
    "pred_data.to_csv(pred_filepath, index=False, header=False)\n",
    "\n",
    "#Save train true vals and predictions to csv\n",
    "\n",
    "pred_data = pd.DataFrame(train_preds)\n",
    "pred_filepath = '../data/predictions/XG/train_pred_xg.csv'\n",
    "pred_data.to_csv(pred_filepath, index=False, header=False)\n",
    "pred_data = pd.DataFrame(y_train)\n",
    "pred_filepath = '../data/predictions/XG/train_true_xg.csv'\n",
    "pred_data.to_csv(pred_filepath, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save inputs to csv\n",
    "\n",
    "pred_data = pd.DataFrame(X_train)\n",
    "pred_filepath = '../data/predictions/XG/train_input_xg.csv'\n",
    "pred_data.to_csv(pred_filepath, index=False, header=False)\n",
    "true_data = pd.DataFrame(X_test)\n",
    "true_filepath = '../data/predictions/XG/test_input_xg.csv'\n",
    "true_data.to_csv(true_filepath, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in values from csv and calculate RMSE and r values\n",
    "\n",
    "test_pred_data = np.genfromtxt('../data/predictions/XG/test_pred_xg.csv', delimiter=',', filling_values=np.nan)\n",
    "test_true_data = np.genfromtxt('../data/predictions/XG/test_true_xg.csv', delimiter=',', filling_values=np.nan)\n",
    "train_pred_data = np.genfromtxt('../data/predictions/XG/train_pred_xg.csv', delimiter=',', filling_values=np.nan)\n",
    "train_true_data = np.genfromtxt('../data/predictions/XG/train_true_xg.csv', delimiter=',', filling_values=np.nan)\n",
    "\n",
    "test_rmse = mean_squared_error(test_true_data,test_pred_data,squared=False)\n",
    "test_r = r2_score(test_true_data,test_pred_data)\n",
    "pearson_r = stats.pearsonr(test_true_data,test_pred_data)\n",
    "\n",
    "train_rmse = mean_squared_error(train_true_data,train_pred_data,squared=False)\n",
    "train_r = stats.pearsonr(train_true_data,train_pred_data)\n",
    "\n",
    "\n",
    "print(\"Train:\")\n",
    "print(train_rmse)\n",
    "print('Test:')\n",
    "print(test_rmse)\n",
    "print(test_r)\n",
    "print(pearson_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"percent Error:\", mean_absolute_percentage_error(test_true_data, test_pred_data)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_df = pd.DataFrame({'true':test_true_data,'pred':test_pred_data})\n",
    "split_df = split_df.sort_values(by='true')\n",
    "split_df.reset_index(inplace=True, drop=True)\n",
    "mid = (max(test_true_data) + min(test_true_data))/2\n",
    "\n",
    "diff = 1000\n",
    "idx = -1\n",
    "for i in range(len(split_df)):\n",
    "    new_diff = abs(split_df.iloc[i]['true'] - mid)\n",
    "    if new_diff <= diff:\n",
    "        diff = new_diff\n",
    "        idx = i\n",
    "print(len(split_df.iloc[idx:]['true'])/len(split_df))\n",
    "top_half_true = split_df.iloc[idx:]['true'].to_numpy().squeeze()\n",
    "top_half_pred = split_df.iloc[idx:]['pred'].to_numpy().squeeze()\n",
    "print(\"adjusted percent Error:\", mean_absolute_percentage_error(top_half_true, top_half_pred)*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
