{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RationalQuadratic, Matern, WhiteKernel, RBF\n",
    "from sklearn.gaussian_process.kernels import Sum\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set this value to true if hyperparameter tuning is complete and the test set should be loaded and predicted on\n",
    "OUTPUT_TEST = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the training and validation datasets\n",
    "X_train = pd.read_csv(\"../data/cleaned/training.csv\")\n",
    "y_train = pd.read_csv(\"../data/cleaned/training_labels.csv\")\n",
    "X_val = pd.read_csv(\"../data/cleaned/validation.csv\")\n",
    "y_val = pd.read_csv(\"../data/cleaned/validation_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some columns headers contain '[' or ']' which are not compatable with sklearn. They are change to '(' and ')' respectively.\n",
    "columns = X_train.columns\n",
    "for col in columns:\n",
    "    if '[' in col or ']' in col:\n",
    "        old_name = col\n",
    "        col = col.replace('[', '(')\n",
    "        col = col.replace(']', ')')\n",
    "        \n",
    "        X_train = X_train.rename(columns={old_name:col})\n",
    "        X_val = X_val.rename(columns={old_name:col})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting of the training set into a vedrification and training set with a 90/10 split. This verification set is used for optuna hyperparameter tuning.\n",
    "X_train, X_verif, y_train, y_verif = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset the indicies after splitting the dataset\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "X_verif = X_verif.reset_index(drop=True)\n",
    "y_verif = y_verif.reset_index(drop=True)\n",
    "X_val = X_val.reset_index(drop=True)\n",
    "y_val = y_val.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the guassian process search space for Optuna.\n",
    "def define_kernel(trial):\n",
    "    kernels = []\n",
    "    n_kernels = trial.suggest_int('n_kernels', 1, 3) #Number of simple kernels used to create the final kernel\n",
    "    for i in range(n_kernels):\n",
    "        kernel_type = trial.suggest_categorical(f'kernel_type_{i}', [\"Matern\", \"RationalQuadratic\", \"WhiteKernel\"]) #Select a type of simple kernel\n",
    "\n",
    "        #Depending on the kernel type selected, certain metrics need to specificed, each of those metrics is selected below by optuna as the tuning takes place\n",
    "        if kernel_type == 'RationalQuadratic':\n",
    "            quad_params = {\n",
    "                'length_scale': trial.suggest_float(f'RationalQuadratic_{i}_length_scale', 1e-5, 1e5),\n",
    "                'alpha': trial.suggest_float(f'RationalQuadratic_{i}_alpha', 1e-5, 1e5)\n",
    "            }   \n",
    "            kernel = RationalQuadratic(length_scale=quad_params['length_scale'], alpha=quad_params['alpha'], length_scale_bounds=(1e-8,1e8))\n",
    "        elif kernel_type == 'Matern':\n",
    "            matern_params = {\n",
    "                'length_scale': trial.suggest_float(f'Matern_{i}_length_scale', 1e-5, 1e5),\n",
    "                'nu': trial.suggest_float(f'Matern_{i}_nu', 0.5, 5)\n",
    "            }\n",
    "            kernel = Matern(length_scale=matern_params['length_scale'], nu=matern_params['nu'], length_scale_bounds=(1e-8,1e8))\n",
    "        elif kernel_type == \"WhiteKernel\":\n",
    "            white_noise_params = {\n",
    "                'noise_level': trial.suggest_float(f'WhiteKernel_{i}_noise_level', 1e-5, 1e5),\n",
    "            }\n",
    "            kernel = WhiteKernel(noise_level=white_noise_params['noise_level'])\n",
    "        else:\n",
    "            print(\"WRONG KERNEL NAME FOR:\", kernel_type)\n",
    "            TypeError\n",
    "        kernels.append(kernel)\n",
    "\n",
    "    if len(kernels) == 1:\n",
    "        return kernels[0]\n",
    "    else:\n",
    "        combined_kernel = Sum(kernels[0], kernels[1])\n",
    "        for j in range(1, n_kernels-1):\n",
    "            combined_kernel = Sum(combined_kernel, kernels[j+1])\n",
    "    return combined_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"Define the objective function\"\"\"\n",
    "    kernel = define_kernel(trial)\n",
    "    params = {\n",
    "        'alpha': trial.suggest_float('alpha', 1e-3, 1e3, log=True),\n",
    "        'n_restarts_optimizer': trial.suggest_int('n_restarts_optimizer', 0, 10),\n",
    "    }\n",
    "\n",
    "    params[\"kernel\"] = kernel\n",
    "    print(params)\n",
    "    # Fit the model\n",
    "    optuna_model = GaussianProcessRegressor(**params)\n",
    "    \n",
    "    batch_size = 500\n",
    "\n",
    "    # Take a random sample of the DataFrame\n",
    "    X_train_sampled = X_train.sample(n=batch_size)\n",
    "\n",
    "    # Access the indexes of the sampled rows\n",
    "    sampled_indexes = X_train_sampled.index\n",
    "    y_train_sampled = y_train.loc[sampled_indexes]\n",
    "\n",
    "    optuna_model.fit(X_train_sampled, y_train_sampled)\n",
    "    \n",
    "    # Make predictions\n",
    "    verif_pred = optuna_model.predict(X_verif)\n",
    "    train_pred = optuna_model.predict(X_train_sampled)\n",
    "    verif_loss = mean_squared_error(y_verif,verif_pred,squared=False)\n",
    "    train_loss = mean_squared_error(y_train_sampled,train_pred,squared=False)\n",
    "\n",
    "\n",
    "    # Evaluate predictions\n",
    "    error = abs(verif_loss - train_loss) + 2 * train_loss\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "study = optuna.create_study(pruner=optuna.pruners.SuccessiveHalvingPruner())\n",
    "study.optimize(objective, n_trials=5)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reconstruct the kernel based on the results from the optuna test.\n",
    "def reconstruct_kernel(encoding):\n",
    "    n_kernels = encoding['n_kernels']\n",
    "    kernels = []\n",
    "    for i in range(n_kernels):\n",
    "        kernel_type = encoding[f'kernel_type_{i}']\n",
    "        length_scale = encoding.get(f'{kernel_type}_{i}_length_scale', None)\n",
    "        nu = encoding.get(f'{kernel_type}_{i}_nu', None)\n",
    "\n",
    "        if kernel_type == 'Matern':\n",
    "            kernel = Matern(length_scale=length_scale, nu=nu)\n",
    "        elif kernel_type == 'RationalQuadratic':\n",
    "            alpha = encoding.get(f'{kernel_type}_{i}_alpha', 1.0)\n",
    "            kernel = RationalQuadratic(length_scale=length_scale, alpha=alpha)\n",
    "        elif kernel_type == 'WhiteKernel':\n",
    "            noise_level = encoding.get(f'{kernel_type}_{i}_noise_level', 1.0)\n",
    "            print(noise_level)\n",
    "            kernel = WhiteKernel(noise_level=noise_level)\n",
    "        # Add more conditions for other kernel types if needed\n",
    "\n",
    "        kernels.append(kernel)\n",
    "\n",
    "    # Sum the individual kernels to get the final composite kernel\n",
    "    if len(kernels) == 1:\n",
    "        return kernels[0]\n",
    "    else:\n",
    "        final_kernel = Sum(kernels[0], kernels[1])\n",
    "        for j in range(1, n_kernels-1):\n",
    "            final_kernel = Sum(final_kernel, kernels[j+1])\n",
    "\n",
    "    return final_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check performance with no tuning to ensure performance is improving\n",
    "sanity_check = GaussianProcessRegressor(kernel=RBF())\n",
    "sanity_check.fit(X_train, y_train)\n",
    "val_pred = sanity_check.predict(X_val)\n",
    "verif_pred = sanity_check.predict(X_verif)\n",
    "sanity_verif_error = mean_squared_error(y_verif,verif_pred,squared=False)\n",
    "sanity_val_error = mean_squared_error(y_val,val_pred,squared=False)\n",
    "print(\"SANITY CHECK VALUES:\")\n",
    "print(\"Verification RMSE:\", sanity_verif_error)\n",
    "print(\"Validation RMSE:\", sanity_val_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = trial.params\n",
    "print(params)\n",
    "kernel = reconstruct_kernel(params)\n",
    "print(kernel)\n",
    "gp = GaussianProcessRegressor(kernel=kernel,alpha=params['alpha'], n_restarts_optimizer=params['n_restarts_optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred, std_prediction = gp.predict(X_val, return_std=True)\n",
    "error = mean_squared_error(y_val,val_pred,squared=False)\n",
    "print(\"RMSE:\", error)\n",
    "print(\"Difference from sanity check:\", sanity_val_error - error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not OUTPUT_TEST:\n",
    "    raise ValueError(\"OUTPUT_TEST set to False. If you would like to output final test values set to True and continue running from here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(\"../data/cleaned/test.csv\")\n",
    "y_test = pd.read_csv(\"../data/cleaned/test_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = X_test.columns\n",
    "for col in columns:\n",
    "    if '[' in col or ']' in col:\n",
    "        old_name = col\n",
    "        col = col.replace('[', '(')\n",
    "        col = col.replace(']', ')')\n",
    "        \n",
    "        X_test = X_test.rename(columns={old_name:col})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = gp.predict(X_test)\n",
    "train_preds = gp.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save test true vals and predictions to csv\n",
    "\n",
    "pred_data = pd.DataFrame(test_preds)\n",
    "pred_filepath = '../data/predictions/GP/test_pred_gp.csv'\n",
    "pred_data.to_csv(pred_filepath, index=False, header=False)\n",
    "pred_data = pd.DataFrame(y_test)\n",
    "pred_filepath = '../data/predictions/GP/test_true_gp.csv'\n",
    "pred_data.to_csv(pred_filepath, index=False, header=False)\n",
    "\n",
    "#Save train true vals and predictions to csv\n",
    "\n",
    "pred_data = pd.DataFrame(train_preds)\n",
    "pred_filepath = '../data/predictions/GP/train_pred_gp.csv'\n",
    "pred_data.to_csv(pred_filepath, index=False, header=False)\n",
    "pred_data = pd.DataFrame(y_train)\n",
    "pred_filepath = '../data/predictions/GP/train_true_gp.csv'\n",
    "pred_data.to_csv(pred_filepath, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save inputs to csv\n",
    "\n",
    "pred_data = pd.DataFrame(X_train)\n",
    "pred_filepath = '../data/predictions/GP/train_input_gp.csv'\n",
    "pred_data.to_csv(pred_filepath, index=False, header=False)\n",
    "true_data = pd.DataFrame(X_test)\n",
    "true_filepath = '../data/predictions/GP/test_input_gp.csv'\n",
    "true_data.to_csv(true_filepath, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in values from csv and calculate RMSE and r values\n",
    "\n",
    "test_pred_data = np.genfromtxt('../data/predictions/GP/test_pred_gp.csv', delimiter=',', filling_values=np.nan)\n",
    "test_true_data = np.genfromtxt('../data/predictions/GP/test_true_gp.csv', delimiter=',', filling_values=np.nan)\n",
    "train_pred_data = np.genfromtxt('../data/predictions/GP/train_pred_gp.csv', delimiter=',', filling_values=np.nan)\n",
    "train_true_data = np.genfromtxt('../data/predictions/GP/train_true_gp.csv', delimiter=',', filling_values=np.nan)\n",
    "\n",
    "test_rmse = mean_squared_error(test_true_data,test_pred_data,squared=False)\n",
    "test_r = stats.pearsonr(test_true_data,test_pred_data)\n",
    "\n",
    "train_rmse = mean_squared_error(train_true_data,train_pred_data,squared=False)\n",
    "train_r = stats.pearsonr(train_true_data,train_pred_data)\n",
    "\n",
    "print(\"Train:\")\n",
    "print(train_rmse)\n",
    "print('Test:')\n",
    "print(test_rmse)\n",
    "print(test_r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
